{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABI - Versão inicial 1\n",
    "\n",
    "(Descrição, ao finalizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1: pré-processamento de texto alternativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 0: importação de bibliotecas\n",
    "\n",
    "Usaremos as seguintes bibliotecas (TODO: adicionar links para a documentação):\n",
    "\n",
    "- Numpy: para vetorização de dados;\n",
    "- Pandas: para manipulação de base dados;\n",
    "- Random: para números aleatórios e seed;\n",
    "- Unidecode: pra processamento de acentos;\n",
    "- NLTK: para ferramentas de NLP;\n",
    "- Re: para processamento de regex;\n",
    "- Scikitlearn: para modelagem estatística e machine learning;\n",
    "- TensorFlow: framework de redes neurais e deep learning;\n",
    "- Keras: API de alto nível para o TensorFlow;\n",
    "- Matplotlib: para plots e visualizações\n",
    "- Seaborn: para plotagem estatística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando de principais bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:16:03.472343Z",
     "start_time": "2020-05-02T01:15:56.991205Z"
    }
   },
   "outputs": [],
   "source": [
    "#processamento de dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#regex\n",
    "import re\n",
    "\n",
    "#nlp\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('rslp')\n",
    "\n",
    "#dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#random\n",
    "import random\n",
    "#pra termos resultados mais fixos\n",
    "\n",
    "#pra tirar acentos\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: leitura de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo a base de perguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:16:03.573510Z",
     "start_time": "2020-05-02T01:16:03.492189Z"
    }
   },
   "outputs": [],
   "source": [
    "#numero de perguntas pra cada intent\n",
    "n = 80\n",
    "\n",
    "df_q = pd.read_csv(\"base_treino_\" + str(n) + \".csv\")\n",
    "\n",
    "#é bom dar uma misturada...\n",
    "df_q = df_q.sample(frac=1, random_state = 42).reset_index(drop=True)\n",
    " \n",
    "#essa é a base desbalanceada\n",
    "df_desbalanc = df_q[~df_q[\"intent\"].isin(df_q[\"intent\"].value_counts()[df_q[\"intent\"].value_counts() == n].index.tolist())]\n",
    "\n",
    "#essa é a base balanceada (vou manter o mesmo nome)\n",
    "df_q = df_q[df_q[\"intent\"].isin(df_q[\"intent\"].value_counts()[df_q[\"intent\"].value_counts() == n].index.tolist())]\n",
    "    \n",
    "if (~df_q[\"intent\"].value_counts().values == n).sum() == 0:\n",
    "    print(\"Bases balanceada e desbalanceadas separadas com sucesso!\")\n",
    "    display(df_q.head())\n",
    "else:\n",
    "    print(\"Erro... Olha o código!\")\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:16:03.642291Z",
     "start_time": "2020-05-02T01:16:03.620967Z"
    }
   },
   "outputs": [],
   "source": [
    "df_q.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:16:05.255943Z",
     "start_time": "2020-05-02T01:16:05.241226Z"
    }
   },
   "outputs": [],
   "source": [
    "perguntas = df_q.iloc[:,1]\n",
    "print(\"\\nTemos\", len(perguntas), \"perguntas na base.\\nAs perguntas estão distribuídas em diferentes intents:\\n\")\n",
    "\n",
    "intents = df_q.iloc[:,0]\n",
    "print(\"Temos\", len(intents.unique()), \"intents na base, com as respectivas quantidades de perguntas\",\n",
    "                                      \"(que idealmente devem ser balanceadas entre os intents):\")\n",
    "display(intents.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:16:06.681258Z",
     "start_time": "2020-05-02T01:16:06.436889Z"
    }
   },
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "#############################################################################################\n",
    "######################################################## PRA PARTE 2:\n",
    "\n",
    "#pra fazer o bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#############################################################################################\n",
    "#############################################################################################\n",
    "######################################################## PRA PARTE 3:\n",
    "\n",
    "#train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#avaliação de performanace\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#Modelos:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#instanciando os modelos\n",
    "modelos = {\"gnb\": GaussianNB(),\n",
    "           \"mnb\": MultinomialNB(),\n",
    "           \"logit\": LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", random_state = 42),\n",
    "           \"svm\": SVC(decision_function_shape='ovo', gamma=\"auto\", kernel=\"linear\", C=10, random_state = 42),\n",
    "           \"rf\": RandomForestClassifier(n_estimators=300, max_depth=None, random_state = 42)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de pré-processamento e modelagem\n",
    "\n",
    "## Passos 2 e 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definição dos modos do pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:26:58.779349Z",
     "start_time": "2020-05-02T01:25:59.590507Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lista de tuplas com os modos do pre-processing\n",
    "# formato: (tira_numeros_string(T/F), stemmer, tira_acento_corpus(T/F), \n",
    "#           tira_acento_antes_ou_depois_do_stemmer, tira stopwords, descrição)\n",
    "\n",
    "modos = [(True, \"RSLPS\", False, \"-\", True, \"sem números, RSLPS, com acento no corpus, tirando stopwords\"),\n",
    "         (False, \"RSLPS\", False, \"-\", True, \"com números, RSLPS, com acento no corpus, tirando stopwords\"),\n",
    "         (True, \"SB\", False, \"-\", True, \"sem números, SB, com acento no corpus, tirando stopwords\"),\n",
    "         (False, \"SB\", False, \"-\", True, \"com números, SB, com acento no corpus, tirando stopwords\"),\n",
    "         \n",
    "         (True, \"RSLPS\", True, \"antes\", True, \"sem números, RSLPS, sem acento no corpus, tira acento antes do stemming, tirando stopwords\"),\n",
    "         (False, \"RSLPS\", True, \"antes\", True, \"com números, RSLPS, sem acento no corpus, tira acento antes do stemming, tirando stopwords\"),\n",
    "         (True, \"SB\", True, \"antes\", True, \"sem números, SB, sem acento no corpus, tira acento antes do stemming, tirando stopwords\"),\n",
    "         (False, \"SB\", True, \"antes\", True, \"com números, SB, sem acento no corpus, tira acento antes do stemming, tirando stopwords\"),\n",
    "         \n",
    "         (True, \"RSLPS\", True, \"depois\", True, \"sem números, RSLPS, sem acento no corpus, tira acento depois do stemming, tirando stopwords\"),\n",
    "         (False, \"RSLPS\", True, \"depois\", True, \"com números, RSLPS, sem acento no corpus, tira acento depois do stemming, tirando stopwords\"),\n",
    "         (True, \"SB\", True, \"depois\", True, \"sem números, SB, sem acento no corpus, tira acento depois do stemming, tirando stopwords\"),\n",
    "         (False, \"SB\", True, \"depois\", True, \"com números, SB, sem acento no corpus, tira acento depois do stemming, tirando stopwords\"), \n",
    "         \n",
    "         \n",
    "         (True, \"RSLPS\", False, \"-\", False, \"sem números, RSLPS, com acento no corpus, sem tirar stopwords\"),\n",
    "         (False, \"RSLPS\", False, \"-\", False, \"com números, RSLPS, com acento no corpus, sem tirar stopwords\"),\n",
    "         (True, \"SB\", False, \"-\", False, \"sem números, SB, com acento no corpus, sem tirar stopwords\"),\n",
    "         (False, \"SB\", False, \"-\", False, \"com números, SB, com acento no corpus, sem tirar stopwords\"),\n",
    "         \n",
    "         (True, \"RSLPS\", True, \"antes\", False, \"sem números, RSLPS, sem acento no corpus, tira acento antes do stemming, sem tirar stopwords\"),\n",
    "         (False, \"RSLPS\", True, \"antes\", False, \"com números, RSLPS, sem acento no corpus, tira acento antes do stemming, sem tirar stopwords\"),\n",
    "         (True, \"SB\", True, \"antes\", False, \"sem números, SB, sem acento no corpus, tira acento antes do stemming, sem tirar stopwords\"),\n",
    "         (False, \"SB\", True, \"antes\", False, \"com números, SB, sem acento no corpus, tira acento antes do stemming, sem tirar stopwords\"),\n",
    "         \n",
    "         (True, \"RSLPS\", True, \"depois\", False, \"sem números, RSLPS, sem acento no corpus, tira acento depois do stemming, sem tirar stopwords\"),\n",
    "         (False, \"RSLPS\", True, \"depois\", False, \"com números, RSLPS, sem acento no corpus, tira acento depois do stemming, sem tirar stopwords\"),\n",
    "         (True, \"SB\", True, \"depois\", False, \"sem números, SB, sem acento no corpus, tira acento depois do stemming, sem tirar stopwords\"),\n",
    "         (False, \"SB\", True, \"depois\", False, \"com números, SB, sem acento no corpus, tira acento depois do stemming, sem tirar stopwords\")]\n",
    "\n",
    "\n",
    "#pra mostrar o corpus antes e após o pré-processamento (caso True)\n",
    "show_preprocess = False\n",
    "\n",
    "#nesta lista eu vou colocar todos os modelos que eu fitar em cada modo, com as respectivas indicações\n",
    "#formato da lista modelos_fitados:\n",
    "#[modo, [nome_do_modelo, modelos_fitados]*numero_de_modelos]\n",
    "modelos_fitados = []\n",
    "\n",
    "#nessa lista eu vou guardar apenas os melhores modelos de cada modo\n",
    "#formato da lista melhores_modelos_fitados\n",
    "#[modo, [nome_do_melhor_modelo, weighted_f1_do_melhor_modelo, melhores_modelos_fitados]*1]\n",
    "melhores_modelos_fitados = []\n",
    "\n",
    "#stopwords\n",
    "stpwrds = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "for (idx, modo) in enumerate(modos):\n",
    "    \n",
    "    modelos_fitados.append([\"Modo: \" + modo[-1]])\n",
    "    print(\"Modo: \" + modo[-1])\n",
    "    \n",
    "    melhores_modelos_fitados.append([\"Modo: \" + modo[-1]])\n",
    "\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #PASSO 2: PRÉ-PROCESSAMENTO\n",
    "\n",
    "    # Etapas de pré-processamento (todas são discutíveis):\n",
    "\n",
    "    # - limpeza do texto: retiramos números e pontuação;\n",
    "    # - deixamos todo o texto em minúsculas;\n",
    "    # - aplicamos stemming. Temos duas opções de stemmers:\n",
    "    #     - RSLP Stemmer: http://www.inf.ufrgs.br/~viviane/rslp/index.htm\n",
    "    #     - Snowball (Porter2): https://snowballstem.org/\n",
    "    # - retiramos as stopwords das perguntas (TODO: talvez reconsiderar e incluir \"não\", entre outras);\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "    #############################################################################################\n",
    "    #############################################################################################\n",
    "    ############################################################################################# \n",
    "    ######################################## CRIA O CORPUS\n",
    "\n",
    "    corpus = []\n",
    "\n",
    "    for item in perguntas:\n",
    "        \n",
    "        #lembrando posições das tuplas de \"modos\":\n",
    "        #modo[0] = tira_numeros_string(T/F)\n",
    "        #modo[1] = stemmer\n",
    "        #modo[2] = tira_acento_corpus(T/F)\n",
    "        #modo[3] = tira_acento_antes_ou_depois_do_stemmer\n",
    "        #modo[4] = tira_stop_words(T/F)\n",
    "        #modo[-1] = descrição\n",
    "        \n",
    "\n",
    "        #vamos jogar fora tudo que não são letras minusculas e maiusculas (incluido acentuações)\n",
    "        #vou manter também números!\n",
    "        \n",
    "        if modo[0]:\n",
    "            #joga fora os numeros tb\n",
    "            pergunta = re.sub(\"[^a-zA-ZáàâãéèêíïóôõöúçñÁÀÂÃÉÈÍÏÓÔÕÖÚÇÑ]\", \" \", item)\n",
    "        else:\n",
    "            #mantem os numeros\n",
    "            pergunta = re.sub(\"[^a-zA-ZáàâãéèêíïóôõöúçñÁÀÂÃÉÈÍÏÓÔÕÖÚÇÑ0-9]\", \" \", item)\n",
    "            \n",
    "\n",
    "        #deixa tudo minuscula\n",
    "        pergunta = pergunta.lower()\n",
    "\n",
    "        #tonkeniza\n",
    "        pergunta = pergunta.split()\n",
    "\n",
    "        if modo[1] == \"RSLPS\":\n",
    "            stemmer = nltk.stem.RSLPStemmer()\n",
    "        elif modo[1] == \"SB\":\n",
    "            stemmer = nltk.stem.SnowballStemmer('portuguese')\n",
    "        else:\n",
    "            print(\"Nenhum stemer foi escolhido!\")\n",
    "            assert(False)\n",
    "\n",
    "        #tira as stopwords ou não\n",
    "        if modo[4]:\n",
    "            #vamos tirar stopwords, e já aplicar o stemmer\n",
    "            if modo[2]:\n",
    "                #também vou tirar todos os acentos com o unidecode\n",
    "                if modo[3] == \"antes\":\n",
    "                    pergunta = [stemmer.stem(unidecode.unidecode(word)) for word in pergunta if word not in set(stpwrds)]\n",
    "                elif modo[3] == \"depois\":\n",
    "                    pergunta = [unidecode.unidecode(stemmer.stem(word)) for word in pergunta if word not in set(stpwrds)]\n",
    "            else:\n",
    "                #nao tiro o acento\n",
    "                pergunta = [stemmer.stem(word) for word in pergunta if word not in set(stpwrds)]\n",
    "        else:\n",
    "            #não tira as stopwords\n",
    "            pergunta = [stemmer.stem(word) for word in pergunta]\n",
    "            \n",
    "        #refaz a string\n",
    "        pergunta = \" \".join(pergunta)\n",
    "\n",
    "        #adiciona ao corpus\n",
    "        corpus.append(pergunta)\n",
    "\n",
    "\n",
    "    if show_preprocess:\n",
    "#         print(\"\\nComparação entre a pergunta original e a versão pré-processada da pergunta no corpus:\\n\")\n",
    "\n",
    "#         for i in range(len(perguntas)):\n",
    "#             print(perguntas.tolist()[i], \"|\", corpus[i])\n",
    "            \n",
    "        print(\"\\nCorpus criado com sucesso!\")\n",
    "    else:\n",
    "        print(\"\\nCorpus criado com sucesso!\")\n",
    "\n",
    "\n",
    "    print(\"\\n___________________________________________________________________\")\n",
    "    print(\"___________________________________________________________________\")\n",
    "    print(\"___________________________________________________________________\\n\")\n",
    "\n",
    "    #############################################################################################\n",
    "    #############################################################################################\n",
    "    ############################################################################################# \n",
    "    ######################################## ANALISA O VOCABULARIO  \n",
    "\n",
    "    #Pra começar, vamos fazer um pequeno estudo quanto ao vocabulário do Corpus\n",
    "\n",
    "    #só pra ter uma ideia do vocabulário, vamos fazer uma lista de listas com o formato:\n",
    "    #vocabulario[i] = [palavra, numero_de_aparicoes_no_corpus]\n",
    "\n",
    "    vocabulario = []\n",
    "    for pergunta in corpus:\n",
    "        for palavra in pergunta.split():\n",
    "            #não queremos palavras de uma única letra (pode acontecer devido ao stemming...)\n",
    "            if len(palavra) > 1:\n",
    "                if palavra not in [x[0] for x in vocabulario]:\n",
    "                    vocabulario.append([palavra, 1])\n",
    "                else:\n",
    "                    vocabulario[[x[0] for x in vocabulario].index(palavra)][1] += 1\n",
    "\n",
    "    print(\"\\nO vocabulário é formado por\", len(vocabulario), \"palavras!\")\n",
    "\n",
    "    #a partir do vocabulário, crio um dataframe com a contagem\n",
    "    vocab_count = pd.DataFrame({\"palavra\": [],\n",
    "                               \"count\": []})\n",
    "\n",
    "    vocab_count[\"palavra\"] = pd.Series(vocabulario).apply(lambda x: x[0])\n",
    "    vocab_count[\"count\"] = pd.Series(vocabulario).apply(lambda x: x[1])\n",
    "    vocab_count = vocab_count.sort_values(\"count\", ascending=False)\n",
    "\n",
    "    if show_preprocess:\n",
    "        print(\"\\nTemos a seguir as 10 mais comuns, com as respectivas contagens:\")\n",
    "        display(vocab_count.head(10))\n",
    "\n",
    "    raras = vocab_count[vocab_count[\"count\"] == 1]\n",
    "    prop_raras = raras.shape[0]/vocab_count.shape[0]\n",
    "    \n",
    "    print(\"\\nA quantidade de palavras com apenas uma aparição no corpus (palavras raras)\\né de \" + str(raras.shape[0]) +\n",
    "         \", o que equivale a\", str(round(100*(prop_raras), 2)) + \"% da base!!\")\n",
    "\n",
    "\n",
    "    print(\"\\n___________________________________________________________________\")\n",
    "    print(\"___________________________________________________________________\")\n",
    "    print(\"___________________________________________________________________\\n\")\n",
    "\n",
    "    #############################################################################################\n",
    "    #############################################################################################\n",
    "    ############################################################################################# \n",
    "    ######################################## FAZ O BAG OF WORDS\n",
    "\n",
    "    #Para fazer o bag of words propriamente, vamos usar o CountVectorizer do sklearn.\n",
    "\n",
    "    #o max_feacutres imita o tamanho do vocabulario.. Não sei o quanto é interessante\n",
    "    # cv = CountVectorizer(max_features = 1500)\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    #features\n",
    "    X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "    #target\n",
    "    # y = intents_num\n",
    "    y = intents.values\n",
    "\n",
    "    #validação\n",
    "    if X.shape[1] == vocab_count.shape[0] and X.shape[0] == len(perguntas):\n",
    "        print(\"As dimensões da matrix de features estão corretas!\")\n",
    "        print(\"\\nBag of words feito com sucesso!\")\n",
    "    else:\n",
    "        print(\"As dimensões não batem! Reveja o código!\")\n",
    "        assert(False)\n",
    "\n",
    "\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    # Parte 3: modelagem preditiva\n",
    "\n",
    "    # Testaremos agora diferentes modelos para a previsão dos intents com base nas perguntas\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "\n",
    "    #############################################################################################\n",
    "    #############################################################################################\n",
    "    ############################################################################################# \n",
    "    ######################################## TRAIN-TEST SPLIT\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify=y)\n",
    "\n",
    "    if show_preprocess:\n",
    "        print(\"Check de balanço do train-test split:\")\n",
    "        display(pd.Series(y_train).value_counts())\n",
    "        display(pd.Series(y_test).value_counts())\n",
    "\n",
    "    #############################################################################################\n",
    "    #############################################################################################\n",
    "    ############################################################################################# \n",
    "    ######################################## MODELOS\n",
    "\n",
    "    #inicializa o melhor f1 como zero\n",
    "    melhor_f1 = 0\n",
    "    for modelo in list(modelos.keys()):\n",
    " \n",
    "        if show_preprocess:\n",
    "            print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "\n",
    "            print(\"Modelo:\", modelo)\n",
    "            print(\"\\nParâmetros:\", modelos[modelo])\n",
    "        \n",
    "        fit = modelos[modelo].fit(X_train, y_train)\n",
    "        y_pred = fit.predict(X_test)\n",
    "\n",
    "        if show_preprocess:\n",
    "            print(\"\\nAvaliação:\\n\")\n",
    "            # print(confusion_matrix(y_test, y_pred))\n",
    "            print(classification_report(y_test, y_pred))\n",
    "        else:\n",
    "            print(\"\\nModelo\", modelo, \"fitado com sucesso!\")\n",
    "        \n",
    "        #salva o modelo fitado na forma modelos_fitados[i] = [nome_do_modelo, modelos_fitados]\n",
    "        modelos_fitados[idx].append([modelo, fit])\n",
    "        \n",
    "        #vendo qual é o melhnor modelo\n",
    "        cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "        f1 = cr[\"weighted avg\"][\"f1-score\"]\n",
    "        \n",
    "        if f1 > melhor_f1:\n",
    "            melhor_f1 = f1\n",
    "            melhor = modelo\n",
    "            melhor_fit = fit\n",
    "\n",
    "    print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "    \n",
    "    print(\"Resumo do modo atual:\")\n",
    "    print(\"Modo:\", modo[-1])\n",
    "    print(\"\\nModelo com maior weighted f1:\", melhor, \", com weighted f1 de\", melhor_f1)\n",
    "    \n",
    "    print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")  \n",
    "    \n",
    "    melhores_modelos_fitados[idx].append([melhor, melhor_f1, modo, melhor_fit])\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    print(\"\\n\\n_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"\\n\\t\\t\\t\\t\\tNOVO MODO\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\")\n",
    "    print(\"_____________________________________________________________________________________________\\n\\n\")\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________\n",
    "    #__________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Resumo visual da performance de cada modo__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:26:58.963564Z",
     "start_time": "2020-05-02T01:26:58.956358Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_melhores = [[y[0], round(y[1], 3)] for y in [x[1] for x in melhores_modelos_fitados]]\n",
    "#foco nas stopwords\n",
    "lista_modos = [[x[0].split(\", \")[-1]] for x in melhores_modelos_fitados]\n",
    "\n",
    "lista_resumo = []\n",
    "for i in range(len(lista_melhores)):\n",
    "    lista_resumo.append(lista_modos[i] + lista_melhores[i])\n",
    "    \n",
    "print(*lista_resumo, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusão 1: tem que tirar stopwords!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:26:59.155147Z",
     "start_time": "2020-05-02T01:26:59.145270Z"
    }
   },
   "outputs": [],
   "source": [
    "#agora vamos desconsiderar os \"sem tirar stopwords\"\n",
    "lista_modos = [[\"|\".join(x[0].split(\": \")[-1].split(\", \")[:-1])] for x in melhores_modelos_fitados if x[0].split(\", \")[-1] != \"sem tirar stopwords\"]\n",
    "lista_melhores = [[y[0], round(y[1],3)] for y in [x[1] for x in melhores_modelos_fitados if x[0].split(\", \")[-1] != \"sem tirar stopwords\"]]\n",
    "\n",
    "lista_resumo = []\n",
    "for i in range(len(lista_melhores)):\n",
    "    lista_resumo.append(lista_modos[i] + lista_melhores[i])\n",
    "    \n",
    "print(*lista_resumo, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusão 2: parece que deixar os números é melhor, e usando o SB, e aparentemente tanto faz tirar o acento antes ou depois do stemming, mas é bom tirar!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "______________\n",
    "______________\n",
    "______________\n",
    "______________\n",
    "______________\n",
    "______________\n",
    "______________\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Obot\n",
    "\n",
    "Vamos pegar o melhor modelo dos que foram treinados acima, e usar aqui pra simular o bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:28:13.242141Z",
     "start_time": "2020-05-02T01:28:13.226663Z"
    }
   },
   "outputs": [],
   "source": [
    "#essa função, será refinada depois. Por enquanto, tô fazendo na mão uma resposta únca\n",
    "def respostas_bot(x):\n",
    "    \n",
    "    respostas = [['Acessórios', 'Olá! O aparelho acompanha fone de ouvido e carregador.'],\n",
    "                 ['Bateria', 'Olá! A bateria é de 3500mAh.'],\n",
    "                 ['Capacidade', 'Olá! A capacidade é de 16Gb.'],\n",
    "                 ['Cor', 'Olá! As cores disponíveis são branco e preto.'],\n",
    "#                  ['Câmera', 'Olá! A câmera é de 64 MP.'],\n",
    "                 ['Dimensão', 'Olá! As dimensões são 15x10x1.'],\n",
    "                 ['Disponibilidade', 'Olá! Produto disponível para pronta entrega!'],\n",
    "                 ['Entrega', 'Olá! Digite seu CEP no canto esquerdo da página para estas informações!'],\n",
    "#                  ['Especificação', 'Olá! O aparelho conta com bluetooth!'],\n",
    "                 ['Estado', 'Olá! O produto é novo.'],\n",
    "                 ['Garantia', 'Olá! O produto tem 3 anos de garantia de fábrica.'],\n",
    "#                  ['Marca', 'Olá! A marca é Xiaomi.'],\n",
    "                 ['Meios de pagamento', 'Olá! Aceitamos boleto e cartão de crédito de todas as bandeiras.'],\n",
    "#                  ['Modelo', 'Olá! O modelo é 23240-XRF.'],\n",
    "                 ['Nota Fiscal', 'Olá! A nota fiscal acompanhará sim o pedido!'],\n",
    "#                  ['Tela', 'Olá! A tela é amoled full HD'],\n",
    "                 ['Voltagem', 'Olá! O produto tem voltagem de 110V']]\n",
    "                 \n",
    "    return respostas[[item[0] for item in respostas].index(x)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:28:16.295967Z",
     "start_time": "2020-05-02T01:28:16.276544Z"
    }
   },
   "outputs": [],
   "source": [
    "#essa é a função de pre-processamento\n",
    "def pre_proc_test(pergunta, modo):\n",
    "    \n",
    "    #lembrando posições das tuplas de \"modos\":\n",
    "    #modo[0] = tira_numeros_string(T/F)\n",
    "    #modo[1] = stemmer\n",
    "    #modo[2] = tira_acento_corpus(T/F)\n",
    "    #modo[3] = tira_acento_antes_ou_depois_do_stemmer\n",
    "    #modo[4] = tira_stop_words(T/F)\n",
    "    #modo[-1] = descrição\n",
    "\n",
    "\n",
    "    #vamos jogar fora tudo que não são letras minusculas e maiusculas (incluido acentuações)\n",
    "    #vou manter também números!\n",
    "\n",
    "    if modo[0]:\n",
    "        #joga fora os numeros tb\n",
    "        pergunta = re.sub(\"[^a-zA-ZáàâãéèêíïóôõöúçñÁÀÂÃÉÈÍÏÓÔÕÖÚÇÑ]\", \" \", pergunta)\n",
    "    else:\n",
    "        #mantem os numeros\n",
    "        pergunta = re.sub(\"[^a-zA-ZáàâãéèêíïóôõöúçñÁÀÂÃÉÈÍÏÓÔÕÖÚÇÑ0-9]\", \" \", pergunta)\n",
    "\n",
    "\n",
    "    #deixa tudo minuscula\n",
    "    pergunta = pergunta.lower()\n",
    "\n",
    "    #tonkeniza\n",
    "    pergunta = pergunta.split()\n",
    "\n",
    "    if modo[1] == \"RSLPS\":\n",
    "        stemmer = nltk.stem.RSLPStemmer()\n",
    "    elif modo[1] == \"SB\":\n",
    "        stemmer = nltk.stem.SnowballStemmer('portuguese')\n",
    "    else:\n",
    "        print(\"Nenhum stemer foi escolhido!\")\n",
    "        assert(False)\n",
    "\n",
    "    #tira as stopwords ou não\n",
    "    if modo[4]:\n",
    "        #vamos tirar stopwords, e já aplicar o stemmer\n",
    "        if modo[2]:\n",
    "            #também vou tirar todos os acentos com o unidecode\n",
    "            if modo[3] == \"antes\":\n",
    "                pergunta = [stemmer.stem(unidecode.unidecode(word)) for word in pergunta if word not in set(stpwrds)]\n",
    "            elif modo[3] == \"depois\":\n",
    "                pergunta = [unidecode.unidecode(stemmer.stem(word)) for word in pergunta if word not in set(stpwrds)]\n",
    "        else:\n",
    "            #nao tiro o acento\n",
    "            pergunta = [stemmer.stem(word) for word in pergunta if word not in set(stpwrds)]\n",
    "    else:\n",
    "        #não tira as stopwords\n",
    "        pergunta = [stemmer.stem(word) for word in pergunta]\n",
    "\n",
    "    #refaz a string\n",
    "    pergunta = \" \".join(pergunta)\n",
    "\n",
    "    return pergunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T02:15:13.773571Z",
     "start_time": "2020-05-02T02:10:47.984867Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def chat(model, modo, th1 = 2):\n",
    "    \n",
    "    print(\"Digite sua pergunta sobre o produto!\")\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        pergunta = input(\"Pergunta: \")\n",
    "        \n",
    "        if pergunta.lower() == \"sair\":\n",
    "            break\n",
    "        \n",
    "        #faz o pre-processing da pergunta\n",
    "        pergunta = pre_proc_test(pergunta, modo)\n",
    "            \n",
    "        #eu tenho que fazer o bag da pergunta usando o mesmo cv e corpus de antes, claro\n",
    "        X = cv.transform([pergunta]).toarray()\n",
    "        \n",
    "        #essas são as probabilidades das classes\n",
    "        class_probs = model.predict_proba(X)\n",
    "        \n",
    "        #essa é a chance random\n",
    "        th2 = 1/len(model.classes_)\n",
    "        \n",
    "#         #ele tenta responder apenas se a chance de estar certo for th vezes maior que a aleatoriedade\n",
    "#         if class_probs.max() > th1*th2:\n",
    "#             y_pred = model.classes_[class_probs.argmax()]\n",
    "#             resposta = respostas_bot(y_pred)\n",
    "# #             print(y_pred)\n",
    "# #             print(class_probs)\n",
    "# #             print(class_probs[abs(class_probs - np.mean(class_probs)) > 1 * np.std(class_probs)])\n",
    "#         else:\n",
    "#             resposta = \"Eu não entendi o que você quis dizer...\"  \n",
    "\n",
    "                \n",
    "\n",
    "        #aqui, eu pego quais são as classes que são mais probáveis\n",
    "        #a forma como eu defino é: classes que têm probabilidade maior do que th1*th2 da probabilidade máxima\n",
    "        #essas são as classes que o algoritmo mais acha que são as corretas\n",
    "        #pode ser que seja só uma, claro\n",
    "        #mas pode ser que seja mais de uma... se for esse o caso, eu dou output de duas classes\n",
    "        class_max = class_probs[class_probs > class_probs.max() - th1*th2]\n",
    "\n",
    "        resposta = []\n",
    "        nao_sei = True\n",
    "        for prob in class_max:\n",
    "            if prob > th1*th2:\n",
    "                y_pred = model.classes_[class_probs.squeeze().tolist().index(prob)]\n",
    "                resposta.append(respostas_bot(y_pred))\n",
    "\n",
    "                nao_sei = False\n",
    "                \n",
    "        #trasnforma as respostas em strings, separadas por \"\\n\"\n",
    "        resposta = \"\\n\".join(resposta)\n",
    "        \n",
    "        #mas, se não houver nenhuma resposta muito provável...\n",
    "        if nao_sei:\n",
    "            resposta = \"Eu não entendi o que você quis dizer...\" \n",
    "        \n",
    "        print(resposta)\n",
    "        \n",
    "        print(\"\\n#################################\\n\")\n",
    "        \n",
    "#lista de lista dos melhores modelos\n",
    "melhores_modelos_lista = [x[1] for x in melhores_modelos_fitados]\n",
    "\n",
    "#pega o indice do melhor modelo (o com maior weighted f1)\n",
    "idx = np.array([x[1] for x in [y[1] for y in melhores_modelos_fitados]]).argmax()\n",
    "\n",
    "#seleciona o melhor modelo\n",
    "melhor_modelo = melhores_modelos_lista[idx][-1]\n",
    "\n",
    "#seleciona o modo do melhor modelo:\n",
    "melhor_modo = melhores_modelos_lista[idx][2]\n",
    "\n",
    "chat(model=melhor_modelo, modo=melhor_modo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
